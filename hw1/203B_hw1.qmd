---
title: "Biostat 203B Homework 1"
subtitle: Due Jan 26, 2024 @ 11:59PM
author: Lingyi Zhang and 606332255
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
knitr:
  opts_chunk: 
    cache: false    
    echo: true
    fig.align: 'center'
    fig.width: 6
    fig.height: 4
    message: FALSE
---

Display machine information for reproducibility:
```{r}
#| eval: false
sessionInfo()
```

## Q1. Git/GitHub

**No handwritten homework reports are accepted for this course.** We work with Git and GitHub. Efficient and abundant use of Git, e.g., frequent and well-documented commits, is an important criterion for grading your homework.

1. Apply for the [Student Developer Pack](https://education.github.com/pack) at GitHub using your UCLA email. You'll get GitHub Pro account for free (unlimited public and private repositories).

2. Create a **private** repository `biostat-203b-2024-winter` and add `Hua-Zhou` and TA team (`Tomoki-Okuno` for Lec 1; `jonathanhori` and `jasenzhang1` for Lec 80) as your collaborators with write permission.

3. Top directories of the repository should be `hw1`, `hw2`, ... Maintain two branches `main` and `develop`. The `develop` branch will be your main playground, the place where you develop solution (code) to homework problems and write up report. The `main` branch will be your presentation area. Submit your homework files (Quarto file `qmd`, `html` file converted by Quarto, all code and extra data sets to reproduce results) in the `main` branch.

4. After each homework due date, course reader and instructor will check out your `main` branch for grading. Tag each of your homework submissions with tag names `hw1`, `hw2`, ... Tagging time will be used as your submission time. That means if you tag your `hw1` submission after deadline, penalty points will be deducted for late submission.

5. After this course, you can make this repository public and use it to demonstrate your skill sets on job market.

## Q2. Data ethics training

This exercise (and later in this course) uses the [MIMIC-IV data v2.2](https://physionet.org/content/mimiciv/2.2/), a freely accessible critical care database developed by the MIT Lab for Computational Physiology. Follow the instructions at <https://mimic.mit.edu/docs/gettingstarted/> to (1) complete the CITI `Data or Specimens Only Research` course and (2) obtain the PhysioNet credential for using the MIMIC-IV data. Display the verification links to your completion report and completion certificate here. 


https://www.citiprogram.org/verify/?k1eb95277-8f4f-4b5f-8ed5-b18220cbc56e-60825135
https://www.citiprogram.org/verify/?wf1fde742-100f-4c6d-94fc-ad613645d9b3-60825135


**You must complete Q2 before working on the remaining questions.** (Hint: The CITI training takes a few hours and the PhysioNet credentialing takes a couple days; do not leave it to the last minute.)

## Q3. Linux Shell Commands

1. Make the MIMIC v2.2 data available at location `~/mimic`. 
```{bash}
ls -l ~/mimic/
```
Refer to the documentation <https://physionet.org/content/mimiciv/2.2/> for details of data files. Please, do **not** put these data files into Git; they are big. Do **not** copy them into your directory. Do **not** decompress the gz data files. These create unnecessary big files and are not big-data-friendly practices. Read from the data folder `~/mimic` directly in following exercises. 

  Use Bash commands to answer following questions.


2. Display the contents in the folders `hosp` and `icu` using Bash command `ls -l`. Why are these data files distributed as `.csv.gz` files instead of `.csv` (comma separated values) files? Read the page <https://mimic.mit.edu/docs/iv/> to understand what's in each folder.
```{bash}
ls -l ~/mimic/hosp
ls -l ~/mimic/icu
```


The question regarding why the MIMIC-IV dataset files are distributed as `.csv.gz` files instead of plain `.csv` files can be answered with the following points:

1. **File Size Reduction**: The `.csv.gz` format indicates that these files are CSV files that have been compressed using the gzip compression algorithm. Compression significantly reduces the size of the files, which is crucial when dealing with a large dataset like MIMIC-IV. This reduction in file size makes it easier and faster to download and transfer the files.

2. **Bandwidth Efficiency**: Compressed files require less bandwidth to download, which is beneficial for users with limited internet speed or data caps, and also reduces the load on the servers hosting the data.

3. **Storage Optimization**: Since the dataset is large, compressing the files helps in saving storage space, both for the hosting service and for the researchers who download the data.

4. **Data Integrity**: Compression can also serve as a way to maintain data integrity during transfer, as the compression and decompression processes include checksums to verify that the files have not been corrupted.

5. **Convenience**: While working with large datasets, it may be more efficient to process the data while it remains compressed. Tools like `zcat`, `zgrep`, and others can be used to work with compressed files directly, avoiding the need to decompress large files on disk.


3. Briefly describe what Bash commands `zcat`, `zless`, `zmore`, and `zgrep` do.

(1). **`zcat`**: 
   - `zcat` is similar to the `cat` command, but it's for compressed files. It displays the contents of compressed files to the standard output (typically the terminal). If you have a file compressed with `gzip` (e.g., `file.gz`), you can view its contents without decompressing the file by using `zcat`.

(2). **`zless`**:
   - `zless` is a file pager for compressed files and works similar to the `less` command. It allows you to view the contents of a compressed file one screen at a time. You can scroll forward and backward through the file with `zless`, which is particularly useful for large files.

(3). **`zmore`**:
   - `zmore` is also a file pager like `zless`, but it's more basic. It allows you to view the contents of a compressed file one screen at a time, but only forward. It's similar to the `more` command but for compressed files.

(4). **`zgrep`**:
   - `zgrep` is used to search inside compressed files. It works just like the `grep` command, which searches files for specified patterns. With `zgrep`, you can search for patterns without having to decompress the file first. It's very useful for searching through large log files that have been compressed.


4. (Looping in Bash) What's the output of the following bash script?
```{bash}
#| eval: false
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  ls -l $datafile
done
```

Display the number of lines in each data file using a similar loop. (Hint: combine linux commands `zcat <` and `wc -l`.)

```{bash}
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  line_count=$(zcat < "$datafile" | wc -l)
  echo "Line Number of $datafile: $line_count"
done

```

5. Display the first few lines of `admissions.csv.gz`. How many rows are in this data file? How many unique patients (identified by `subject_id`) are in this data file? Do they match the number of patients listed in the `patients.csv.gz` file? (Hint: combine Linux commands `zcat <`, `head`/`tail`, `awk`, `sort`, `uniq`, `wc`, and so on.)


```{bash}
echo "First few lines of admissions.csv.gz:"
zcat <  /Users/a1234/mimic/hosp/admissions.csv.gz | head
```






```{bash}
echo -n "Number of rows in admissions.csv.gz: "
zcat <  /Users/a1234/mimic/hosp/admissions.csv.gz | wc -l
```



```{bash}
echo -n "Number of unique patients in admissions.csv.gz: "
zcat < /Users/a1234/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F',' '{print $1}' | sort -u | wc -l
```



```{bash}
echo -n "Number of patients in patients.csv.gz: " 
zcat < /Users/a1234/mimic/hosp/patients.csv.gz | tail -n +2 | awk -F',' '{print $1}' | sort -u | wc -l
```
Number of patients in patients.csv.gz:   299712
No they don't match.




6. What are the possible values taken by each of the variable `admission_type`, `admission_location`, `insurance`, and `ethnicity`? Also report the count for each unique value of these variables. (Hint: combine Linux commands `zcat`, `head`/`tail`, `awk`, `uniq -c`, `wc`, and so on; skip the header line.)



```{bash}
echo "Possible values and counts for admission_type:";  
zcat < /Users/a1234/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F',' '{print $6}' | sort | uniq -c 
```




```{bash}
echo "Possible values and counts for admission_location:";  
zcat < /Users/a1234/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F',' '{print $8}' | sort | uniq -c 
```




```{bash}
echo "Possible values and counts for insurance:";  
zcat < /Users/a1234/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F',' '{print $10}' | sort | uniq -c 
```




```{bash}
echo "Possible values and counts for race:";  
zcat < /Users/a1234/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F',' '{print $13}' | sort | uniq -c
```







7. _To compress, or not to compress. That's the question._ Let's focus on the big data file `labevents.csv.gz`. Compare compressed gz file size to the uncompressed file size. Compare the run times of `zcat < ~/mimic/labevents.csv.gz | wc -l` versus `wc -l labevents.csv`. Discuss the trade off between storage and speed for big data files. (Hint: `gzip -dk < FILENAME.gz > ./FILENAME`. Remember to delete the large `labevents.csv` file after the exercise.)

```{bash}
ls -lh /Users/a1234/mimic/hosp/labevents.csv.gz
```
1.8G	labevents.csv.gz



```{bash}
gzip -dk < /Users/a1234/mimic/hosp/labevents.csv.gz > /Users/a1234/mimic/hosp/labevents_uncompressed.csv
ls -lh /Users/a1234/mimic/hosp/labevents_uncompressed.csv
```
13G	labevents.csv

so the compressed file is 1.8G and uncompressed file is 13G. The compressed file is much smaller than the uncompressed file. 

```{bash}
time zcat < /Users/a1234/mimic/hosp/labevents.csv.gz | wc -l
```



```{bash}
time wc -l /Users/a1234/mimic/hosp/labevents_uncompressed.csv
```
When deciding on the compression of large data files, the particular use case and requirements are crucial considerations. If storage space is limited and there's sufficient computational power to manage the overhead of decompression, opting for compression can be beneficial. Conversely, in circumstances where immediate data access is paramount and storage constraints are of lesser importance, it might be more advantageous to maintain the files in an uncompressed state.





## Q4. Who's popular in Price and Prejudice

1. You and your friend just have finished reading *Pride and Prejudice* by Jane Austen. Among the four main characters in the book, Elizabeth, Jane, Lydia, and Darcy, your friend thinks that Darcy was the most mentioned. You, however, are certain it was Elizabeth. Obtain the full text of the novel from <http://www.gutenberg.org/cache/epub/42671/pg42671.txt> and save to your local folder. 
```{bash}
#| eval: false
wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt
```
Explain what `wget -nc` does. Do **not** put this text file `pg42671.txt` in Git. Complete the following loop to tabulate the number of times each of the four characters is mentioned using Linux commands.
```{bash}
#| eval: false
wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt
for char in Elizabeth Jane Lydia Darcy
do
  echo $char:
  # some bash commands here
done
```
Explanation:
 wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt: Downloads the text file if it doesn't exist locally (-nc prevents overwriting).
 for char in Elizabeth Jane Lydia Darcy: Iterates over the specified characters.
 echo "$char:": Prints the current character name.
 count=$(grep -o -i $char pg42671.txt | wc -l): Uses grep to find occurrences of the character (case-insensitive -i), and wc -l counts the number of lines (occurrences).
 echo "Occurrences: $count": Prints the count for the current character.








2. What's the difference between the following two commands?
```{bash}
#| eval: false
echo 'hello, world' > test1.txt
```
and
```{bash}
#| eval: false
echo 'hello, world' >> test2.txt
```
The two commands use different redirection operators (`>` and `>>`) in the Unix/Linux shell, and they function differently when outputting the results of a command to a file:

 `echo 'hello, world' > test1.txt`:
   - This command uses the `>` operator, which redirects the output of the `echo` command to the file `test1.txt`. If `test1.txt` does not exist, it will be created. If `test1.txt` does exist, **its contents will be overwritten** without warning. In essence, the `>` operator is used for writing output to a file, but it does not preserve the contents of the file if it already exists.

 `echo 'hello, world' >> test2.txt`:
   - This command uses the `>>` operator, which also redirects the output of the `echo` command to the file `test2.txt`. However, if `test2.txt` does exist, the `echo` command's output is **appended** to the end of the file, preserving the existing contents. If `test2.txt` does not exist, it will be created. The `>>` operator is used when you want to add to a file's contents rather than overwriting them.

So, the key difference is that `>` overwrites the entire file, whereas `>>` appends to the end of the file, keeping the existing contents intact.






3. Using your favorite text editor (e.g., `vi`), type the following and save the file as `middle.sh`:
```{bash eval=FALSE}
#!/bin/sh
# Select lines from the middle of a file.
# Usage: bash middle.sh filename end_line num_lines
head -n "$2" "$1" | tail -n "$3"
```
Using `chmod` to make the file executable by the owner, and run
```{bash}
#| eval: false
./middle.sh pg42671.txt 20 5
```
Explain the output. Explain the meaning of `"$1"`, `"$2"`, and `"$3"` in this shell script. Why do we need the first line of the shell script?
"$1": Refers to the first command-line argument, which is the filename passed when running the script (pg42671.txt in your example).
"$2": Refers to the second command-line argument, which is the end line from where to start selecting lines.
"$3": Refers to the third command-line argument, which is the number of lines to select.

The first line (#!/bin/sh) is necessary to specify the interpreter for the script. It tells the system that the script should be executed using the Bourne shell (/bin/sh in this case). Without this line, the script might be executed by a default shell, and unexpected behavior could occur if the syntax used in the script is not compatible with that shell. In this script, the actual commands use Bash syntax, so the shebang line should ideally be changed to #!/bin/bash.









## Q5. More fun with Linux

Try following commands in Bash and interpret the results: `cal`, `cal 2024`, `cal 9 1752` (anything unusual?), `date`, `hostname`, `arch`, `uname -a`, `uptime`, `who am i`, `who`, `w`, `id`, `last | head`, `echo {con,pre}{sent,fer}{s,ed}`, `time sleep 5`, `history | tail`.

cal # Displays the calendar for the current month.
cal 2024 # Displays the calendar for the year 2024.
cal 9 1752 # This command displays the calendar for September 1752. Something unusual might be observed in this output. In September 1752, the Gregorian calendar was adopted, and 11 days were skipped to align the calendar with the astronomical year. Therefore, the output will show the dates from September 3 to September 13, 1752, missing the days from September 3 to September 13, 1752.
date # Displays the current date and time.
hostname # Displays the host (computer) name.
arch # Displays the computer architecture. It shows the architecture of the machine, such as x86_64.
uname -a # Displays detailed information about the system, including the kernel version, architecture, and other details.
uptime # Shows how long the system has been running and the system load.
who am i # Displays information about the current user and terminal.
who # Shows a list of currently logged-in users.
w # Provides information about currently logged-in users and system load.
id # Shows the user and group IDs for the current user.
last | head # Displays a list of recent logins, and head shows only the first few lines.
echo {con,pre}{sent,fer}{s,ed} # Expands and prints a combination of words. It produces variations like "consents," "confers," "prevents," "prefers," and so on.
time sleep 5 # Measures the time it takes to execute the sleep 5 command. This is useful for benchmarking and timing commands.
history | tail # Displays the command history, and tail shows the last few lines of the history.




## Q6. Book

1. Git clone the repository <https://github.com/christophergandrud/Rep-Res-Book> for the book _Reproducible Research with R and RStudio_ to your local machine. 

2. Open the project by clicking `rep-res-3rd-edition.Rproj` and compile the book by clicking `Build Book` in the `Build` panel of RStudio. (Hint: I was able to build `git_book` and `epub_book` but not `pdf_book`.)

The point of this exercise is (1) to get the book for free and (2) to see an example how a complicated project such as a book can be organized in a reproducible way.

For grading purpose, include a screenshot of Section 4.1.5 of the book here.